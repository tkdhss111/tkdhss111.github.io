---
title: '変数選択法'
subtitle: 'データサイエンスコース'
author: 'Hisashi Takeda, Ph.D.'
date: '2025-03-04'
---

# 変数選択法とは

## 変数選択

本来不要な説明変数が多いと，それぞれの説明変数の影響が重なり合い思わぬ結果がでしまうことがある。

不要な説明変数はできるだけ除くことが望ましい。
変数選択（特徴量選択）には主に次の３つの方法がある。

* **変数指定法**

ドメイン知識により必要な説明変数を指定する方法 （最も分かりやすい）

* **総当り法**

全ての説明変数の組み合わせを網羅し
最も良い説明変数を選択する方法。 （計算コストが高い）

* **逐次選択法**

一定の規則に従って説明変数を逐次選択する方法。  
（計算コストを抑えつつ自動で選択する）

# 逐次選択法の種類

[^統計学がわかった！]: [統計学がわかった！](https://toukeigaku-jouhou.info/2019/07/20/variable-selection-procedure)

##  フォワード法

![](fig/forward.png)[^統計学がわかった！]

##  バックワード法

![](fig/backward.png)[^統計学がわかった！]

## ステップワイズ法

![](fig/stepwise.png)[^統計学がわかった！]

## 変数選択法利用時の注意点

変数選択法は，全く本来因果関係のない変数が選択される場合があり
合理的な解釈に苦しむことがある。

理想的には，ドメイン知識（専門知識）をもとに回帰分析前に
必要となる説明変数を決める方が良い。

しかし，総当り法や逐次選択法では分析者が今まで気づかなかった有効な変数や特徴量を発見することもある。

**変数選択が不必要／必要な場合**

![](fig/why_variable_selection.png)[^総務省統計局第２講]

[^総務省統計局第２講]: [総務省統計局
第２講](https://www.stat.go.jp/teacher/dl/pdf/c3learn/materials/third/dai2.pdf)

## 汎化性能

未知のデータに対する予測能力のことを汎化性能^[はんかせいのう]という。

過学習モデルは，この性能が低い。

いかに訓練データに過学習することなく汎化性能を高めるかが
良い予測モデル作成のためにとても重要である。

データサイエンティストの腕の見せ所でもある。

# その他の変数選択手法

## スパース線形回帰

回帰係数の推定と変数選択を同時に行う効率的な手法

LASSO^[ラッソ]回帰が有名。

LASSO回帰では予測に寄与しない説明変数の回帰係数がゼロになる（スパース性）
ことで変数選択を自動で行う。

## ランダムフォレスト

::::: columns
::: {.column width="50%"}

ランダムフォレスト^[random forest]とは，ランダムサンプリングされた訓練データから作られた複数の決定木で説明変数をランダムに選択したものを弱学習器とするアンサンブル学習アルゴリズム。

:::
::: {.column width="50%"}

![](fig/random_forest.png)[^【Wikipedia】ランダムフォレスト]

[^【Wikipedia】ランダムフォレスト]: [【Wikipedia】ランダムフォレスト](https://ja.wikipedia.org/wiki/ランダムフォレスト)

:::
:::::

# 変数選択法の適用例

車の燃費データを使って回帰分析する。

## データ

単位はオリジナル（出典参照）から馴染みのあるものに変換した。

説明変数|内容（単位）
--------|-------------------------
    燃費|燃費（km/L）
  気筒数|気筒数（本）
  排気量|排気量（cc）
    馬力|馬力（hp）
車体重量|車体重量（kg）
加速性能|加速性能：時速97km（60mile）に到達する時間（秒）
  製造年|製造年（年）
    分類|アメ車，日本車，欧州車の3区分
    車名|車名（英語）

: [【UCI Machine Learning Repository】Auto MPG Data Set](https://archive.ics.uci.edu/ml/datasets/Auto+MPG)

```{r}
d <- read.csv('https://stats.dip.jp/01_ds/data/car_mileage.csv')
rownames(d) <- paste0('No.', 1:nrow(d))

library(DT)
datatable(d)
```

## フィッティング

- 車名を除くすべての説明変数を使って回帰分析する。

- 車名を入れると変数の数がデータサイズに近づき過学習するので注意

```{r message=FALSE}
fit.full <- lm(燃費 ~ 気筒数 + 排気量 + 馬力 + 車体重量 + 加速性能 + 製造年 + 分類, data = d)
#summary(fit.full)
library(sjPlot)
tab_model(fit.full, show.stat = T, show.aic = T)

```

## 変数を手動選択 

有意でない偏回帰係数を持つ説明変数を除外

```{r}
fit.manual <- lm(燃費 ~ 排気量 + 車体重量 + 製造年 + 分類, data = d)
#summary(fit.manual)
tab_model(fit.manual, show.stat = T, show.aic = T)
```

## ステップワイズ変数選択

変数間の相乗効果を表す**交互作用項**を追加（モデル式を2乗）する。
交互変数を追加すると組み合わせ数が多くなり，手動では変数選択が難しいため，
ステップワイズ変数選択法を使用する。

```{r}
library(MASS)
fit.aic <- stepAIC(lm(燃費 ~ (気筒数 + 排気量 + 馬力 + 車体重量 + 加速性能 + 製造年 + 分類)^2, data = d), trace = 0)
#summary(fit.aic)
tab_model(fit.aic, show.stat = T, show.aic = T)
```

---

### 注意点
ステップワイズ変数選択では有意でない偏回帰係数も選択される。
これはモデル選択の基準となるAICが予測誤差最小を評価する指標であり，
偏回帰係数の有意性の有無は考慮していないためである。  

## モデル比較

```{r}
tab_model(fit.full, fit.manual, fit.aic, 
          show.stat = T, show.aic = T,
          dv.labels = c('車名除きフルモデル',
                        '手動選択モデル', 
                        'ステップワイズ変数選択モデル'))
```

>自由度調整済み決定係数（$R^2$ adjusted）やAICを比べると，
>は**ステップワイズ変数選択モデル**，次いで**車名除きフルモデル**，
>最後に**手動選択モデル**の順に良いモデルとなった。

## 演習課題

1. 赤ワインの品質（quality）を予測する重回帰モデルとして，
フルモデル，手動選択モデル，ステップワイズ変数選択モデルを作成し性能を評価せよ。  

2. ステップワイズ変数選択で，
化学物質の組み合わせの妙を表現するための交互作用項を導入せよ。

データの内容は
[赤ワインデータの解析](https://www.statlab.co.jp/seminar/redwine01.html)
を参照のこと。

```{r}
d <- read.csv('https://stats.dip.jp/01_ds/data/winequality-red.csv')
datatable(d)
```
